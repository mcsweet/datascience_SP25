mutate(prevalence_data = name_prevalence()) %>%
mutate(estimates = prevalence_data$estimate)
}
)
) %>%
## NOTE: No need to edit this line; this uses your bootstrap sample to compute
# a confidence `int`erval using the percentile method
int_pctl(., estimate)
# TASK: Complete the code below to compute a bootstrap-based confidence interval
df_interval_bootstrap <-
df_sample_random %>%
bootstraps(., times = 1000) %>%
mutate(
estimate = map(
splits,
function(split_df) {
## TODO: Finish this code, using the name_prevalence() helper you implemented
## HINT: Remember that you need to use analysis() when operating on split_df
analysis(split_df) %>%
mutate(prevalence_data = name_prevalence(., last = "Collins")) %>%
mutate(estimates = prevalence_data$estimate)
}
)
) %>%
## NOTE: No need to edit this line; this uses your bootstrap sample to compute
# a confidence `int`erval using the percentile method
int_pctl(., estimate)
# TASK: Complete the code below to compute a bootstrap-based confidence interval
df_interval_bootstrap <-
df_sample_random %>%
bootstraps(., times = 1000) %>%
mutate(
estimate = map(
splits,
function(split_df) {
## TODO: Finish this code, using the name_prevalence() helper you implemented
## HINT: Remember that you need to use analysis() when operating on split_df
analysis(split_df) %>%
summarise(prevalence_data = name_prevalence(., last = "Collins")) %>%
summarise(estimates = prevalence_data$estimate)
}
)
) %>%
## NOTE: No need to edit this line; this uses your bootstrap sample to compute
# a confidence `int`erval using the percentile method
int_pctl(., estimate)
# TASK: Complete the code below to compute a bootstrap-based confidence interval
df_interval_bootstrap <-
df_sample_random %>%
bootstraps(., times = 1000) %>%
mutate(
estimate = map(
splits,
function(split_df) {
## TODO: Finish this code, using the name_prevalence() helper you implemented
## HINT: Remember that you need to use analysis() when operating on split_df
analysis(split_df) %>%
prevalence_data <- name_prevalence(., last = "Collins"))
# TASK: Complete the code below to compute a bootstrap-based confidence interval
df_interval_bootstrap <-
df_sample_random %>%
bootstraps(., times = 1000) %>%
mutate(
estimate = map(
splits,
function(split_df) {
## TODO: Finish this code, using the name_prevalence() helper you implemented
## HINT: Remember that you need to use analysis() when operating on split_df
analysis(split_df) %>%
prevalence_data <- name_prevalence(., last = "Collins")
estimates <- prevalence_data$estimate
}
)
) %>%
## NOTE: No need to edit this line; this uses your bootstrap sample to compute
# a confidence `int`erval using the percentile method
int_pctl(., estimate)
# TASK: Complete the code below to compute a bootstrap-based confidence interval
df_interval_bootstrap <-
df_sample_random %>%
bootstraps(., times = 1000) %>%
mutate(
estimate = map(
splits,
function(split_df) {
## TODO: Finish this code, using the name_prevalence() helper you implemented
## HINT: Remember that you need to use analysis() when operating on split_df
analysis(split_df) %>%
name_prevalence(., last = "Collins")
}
)
) %>%
## NOTE: No need to edit this line; this uses your bootstrap sample to compute
# a confidence `int`erval using the percentile method
int_pctl(., estimate)
df_interval_bootstrap
{r q2-task}
# TASK: Complete the code below to compute a bootstrap-based confidence interval
df_interval_bootstrap <-
df_sample_random %>%
bootstraps(., times = 1000) %>%
mutate(
estimate = map(
splits,
function(split_df) {
## TODO: Finish this code, using the name_prevalence() helper you implemented
## HINT: Remember that you need to use analysis() when operating on split_df
analysis(split_df) %>%
name_prevalence(., last = "Collins")
}
)
) %>%
## NOTE: No need to edit this line; this uses your bootstrap sample to compute
# a confidence `int`erval using the percentile method
int_pctl(., estimate)
df_interval_bootstrap
# TASK: Complete the code below to compute a bootstrap-based confidence interval
df_interval_bootstrap <-
df_sample_random %>%
bootstraps(., times = 1000) %>%
mutate(
estimate = map(
splits,
function(split_df) {
## TODO: Finish this code, using the name_prevalence() helper you implemented
## HINT: Remember that you need to use analysis() when operating on split_df
analysis(split_df) %>%
name_prevalence(., last = "Collins")
}
)
) %>%
## NOTE: No need to edit this line; this uses your bootstrap sample to compute
# a confidence `int`erval using the percentile method
int_pctl(., estimate)
df_interval_bootstrap
# TASK: Complete the code below to compute a bootstrap-based confidence interval
df_interval_bootstrap <-
df_sample_random %>%
bootstraps(., times = 1000) %>%
mutate(
estimate = map(
splits,
function(split_df) {
## TODO: Finish this code, using the name_prevalence() helper you implemented
## HINT: Remember that you need to use analysis() when operating on split_df
analysis(split_df) %>%
name_prevalence(., last = "Regan")
}
)
) %>%
## NOTE: No need to edit this line; this uses your bootstrap sample to compute
# a confidence `int`erval using the percentile method
int_pctl(., estimate)
df_interval_bootstrap
## NOTE: No need to change this
## Check that data has `p` column and is in descending order
assertthat::assert_that(
all(df_q3 %>%
mutate(d = p - lead(p)) %>%
filter(!is.na(d)) %>%
pull(d) >= 0
)
)
print("Very good!")
## TASK: Set the parameters for this code block
## Select a random sample of houses
n_houses <- 554
n_sample <- 25
set.seed(101)   # Set a seed for reproducibility
df_numbers_random <-
tibble(
house = sample(
1:n_houses,     # All integers from 1 to n_houses
n_sample,       # Size of our sample
replace = FALSE # Sample *WITHOUT* replacement
)
) %>%
# Arrange for our data collection convenience
arrange(house)
# Pull the column so we can list just the house numbers
df_numbers_random %>%
pull(house)
library(tidyverse)
library(rsample)
filename_random <- "./data/helvig-random.csv"
## NOTE: Do not edit this
df_sample_seq <-
read_csv("./data/helvig-seq.csv")%>%
drop_na(name)
df_sample_seq
## TASK: Compute the prevalence and sort
df_q3 <-
df_sample_seq %>%
## TODO: Complete this code
mutate(last_name = sapply(strsplit(as.character(name)," "), function(x) tail(x, 1))) %>%
group_by(last_name) %>%
summarise(
n = n(),
p = n() / nrow(.)
) %>%
arrange(desc(p))
df_q3
#this was how I figued out how to do this math. I just saw how you did it below which makes sense and is much simpler. I am keeping it the way that I did it to show you my own work.
## NOTE: No need to change this
## Check that data has `p` column and is in descending order
assertthat::assert_that(
all(df_q3 %>%
mutate(d = p - lead(p)) %>%
filter(!is.na(d)) %>%
pull(d) >= 0
)
)
print("Very good!")
## TASK: Set the parameters for this code block
## Select a random sample of houses
n_houses <- 554
n_sample <- 25
set.seed(101)   # Set a seed for reproducibility
df_numbers_random <-
tibble(
house = sample(
1:n_houses,     # All integers from 1 to n_houses
n_sample,       # Size of our sample
replace = FALSE # Sample *WITHOUT* replacement
)
) %>%
# Arrange for our data collection convenience
arrange(house)
# Pull the column so we can list just the house numbers
df_numbers_random %>%
pull(house)
## NOTE: No need to change this
assertthat::assert_that(
all(dim(df_numbers_random) == c(25, 1))
)
## NOTE: Do not edit
filename_random
## NOTE: Do not edit
df_sample_random <-
read_csv(filename_random) %>%
select(-4)
df_sample_random
## NOTE: No need to change this
# Check that the dataset has the correct column names
assertthat::assert_that(setequal(
df_sample_random %>% names(),
df_sample_seq %>% names()
))
# Check that all of the house numbers in the dataset match those that were planned
numVsamp <-
anti_join(
df_numbers_random,
df_sample_random %>% distinct(house),
by = "house"
) %>%
pull(house)
assertthat::assert_that(
length(numVsamp) == 0,
msg = str_c("You are missing the houses: ", numVsamp)
)
sampVnum <-
anti_join(
df_sample_random %>% distinct(house),
df_numbers_random,
by = "house"
) %>%
pull(house)
assertthat::assert_that(
length(sampVnum) == 0,
msg = str_c("You have extra houses: ", sampVnum)
)
print("Great work!")
# NOTE: No need to edit; run and answer the questions below
df_sample_random %>%
mutate(last = str_extract(name, "\\w+$")) %>%
count(last) %>%
arrange(desc(n)) %>%
mutate(p = n / sum(n))
## TASK: Write a helper function that takes a dataframe with full names
#  (provided in a `name` column), removes any invalid rows, and computes the
#  proportion of individuals with the user-specified `last` name (returned
#  in an `estimate` column).
name_prevalence <- function(df, last = "Collins") {
df %>%
mutate(last_name = str_extract(name, "\\w+$")) %>%
drop_na(last_name) %>%
count(last_name) %>%
mutate(p = n / sum(n)) %>%
filter(last_name == last) %>%
summarise(
term = last_name,
estimate = p
)
}
## NOTE: No need to change this
# Find the most prevalent name in the data
last_most <-
df_sample_random %>%
mutate(last = str_extract(name, "\\w+$")) %>%
count(last) %>%
arrange(desc(n)) %>%
slice(1) %>%
pull(last)
# Ensure correct columns
assertthat::assert_that(
setequal(
tibble(name = c("James")) %>% name_prevalence(., last = "James") %>% names(),
c("term", "estimate")
),
msg = "Your code should result a dataframe with just two columns: `term` and `estimate`"
)
# Ensure NA handling
assertthat::assert_that(
!(tibble(name = c(NA_character_, "James")) %>%
name_prevalence(., last = "James") %>%
pull(estimate) %>%
is.na()),
msg = "Ensure your code properly ignores NA's"
)
# Check for correctness
assertthat::assert_that(
name_prevalence(df_sample_random, last = last_most) %>% pull(estimate) ==
mean(str_detect(df_sample_random$name, last_most), na.rm = TRUE),
msg = "Your code computed the wrong value"
)
print("Nice!")
# TASK: Complete the code below to compute a bootstrap-based confidence interval
df_interval_bootstrap <-
df_sample_random %>%
bootstraps(., times = 1000) %>%
mutate(
estimate = map(
splits,
function(split_df) {
## TODO: Finish this code, using the name_prevalence() helper you implemented
## HINT: Remember that you need to use analysis() when operating on split_df
analysis(split_df) %>%
name_prevalence(., last = "Regan")
}
)
) %>%
## NOTE: No need to edit this line; this uses your bootstrap sample to compute
# a confidence `int`erval using the percentile method
int_pctl(., estimate)
df_interval_bootstrap
## TASK: Compute the prevalence and sort
df_q3 <-
df_sample_seq %>%
## TODO: Complete this code
mutate(last_name = sapply(strsplit(as.character(name)," "), function(x) tail(x, 1))) %>%
group_by(last_name) %>%
summarise(
n = n(),
p = n() / nrow(.)
) %>%
arrange(desc(p))
df_q3
#this was how I figued out how to do this math. I just saw how you did it below which makes sense and is much simpler. I am keeping it the way that I did it to show you my own work.
## NOTE: No need to change this
## Check that data has `p` column and is in descending order
assertthat::assert_that(
all(df_q3 %>%
mutate(d = p - lead(p)) %>%
filter(!is.na(d)) %>%
pull(d) >= 0
)
)
#print("Very good!")
library(tidyverse)
## TODO: Give the filename for your copy of Table B01003
filename_pop <- "./data/Pop_Data.csv"
## NOTE: No need to edit
df_pop <-
read_csv(
filename_pop,
skip = 1,
) %>%
rename(
population_estimate = `Estimate!!Total`
)
df_pop
## TODO: Give the filename for your copy of Table S1903
filename_income <- "./data/income.csv"
## NOTE: No need to edit
df_income <-
read_csv(filename_income, skip = 1)
df_income
## NOTE: No need to edit, use to check you got the right file.
assertthat::assert_that(
df_income %>%
filter(Geography == "0500000US01001") %>%
pull(`Estimate!!Percent Distribution!!FAMILY INCOME BY FAMILY SIZE!!2-person families`)
== 45.6
)
print("Well done!")
df_q3 <-
df_income %>%
select(
Geography,
contains("Geographic"),
# This will select only the numeric d-person family columns;
# it will ignore the annotation columns
contains("median") & matches("\\d-person families") & !contains("Annotation of")
) %>%
mutate(across(contains("median"), as.numeric)) %>%
## TODO: Pivot the data, rename the columns
pivot_longer(
names_pattern = "(Estimate|Margin of Error).*(\\d-person families).*",
names_to = c(".value", "category"),
cols = matches("(Estimate|Margin of Error)")
) %>%
rename("income_estimate" = "Estimate",
"income_moe" = "Margin of Error",
"geographic_area_name" = "Geographic Area Name") %>%
glimpse()
## NOTE: No need to edit
assertthat::assert_that(setequal(
names(df_q3),
c("Geography", "geographic_area_name", "category", "income_estimate", "income_moe")
))
assertthat::assert_that(
df_q3 %>%
filter(Geography == "0500000US01001", category == "2-person families") %>%
pull(income_moe)
== 6663
)
print("Nice!")
df_q4 <-
df_q3 %>%
mutate(income_SE = income_moe / 1.645,
income_CV = income_SE / income_estimate,
income_lo = income_estimate - 2.576 * income_SE,
income_hi = income_estimate + 2.576 * income_SE)
## NOTE: No need to edit
assertthat::assert_that(setequal(
names(df_q4),
c("Geography", "geographic_area_name", "category", "income_estimate", "income_moe",
"income_SE", "income_lo", "income_hi", "income_CV")
))
assertthat::assert_that(
abs(
df_q4 %>%
filter(Geography == "0500000US01001", category == "2-person families") %>%
pull(income_SE) - 4050.456
) / 4050.456 < 1e-3
)
assertthat::assert_that(
abs(
df_q4 %>%
filter(Geography == "0500000US01001", category == "2-person families") %>%
pull(income_lo) - 54513.72
) / 54513.72 < 1e-3
)
assertthat::assert_that(
abs(
df_q4 %>%
filter(Geography == "0500000US01001", category == "2-person families") %>%
pull(income_hi) - 75380.28
) / 75380.28 < 1e-3
)
assertthat::assert_that(
abs(
df_q4 %>%
filter(Geography == "0500000US01001", category == "2-person families") %>%
pull(income_CV) - 0.06236556
) / 0.06236556 < 1e-3
)
print("Nice!")
## TODO: Join df_q4 and df_pop by the appropriate column
df_data <-
left_join(df_pop, df,q4, b = "Geography")
df_q4 <-
df_q3 %>%
mutate(income_SE = income_moe / 1.645,
income_CV = income_SE / income_estimate,
income_lo = income_estimate - 2.576 * income_SE,
income_hi = income_estimate + 2.576 * income_SE)
## TODO: Join df_q4 and df_pop by the appropriate column
df_data <-
left_join(df_pop, df_q4, b = "Geography")
## TODO: Join df_q4 and df_pop by the appropriate column
df_data <-
left_join(df_pop, df_q4, b = "Geography")
## TODO: Join df_q4 and df_pop by the appropriate column
df_data <-
left_join(df_pop, df_q4, b = "Geography")
## NOTE: No need to edit; run and inspect
wid <- 0.5
df_data %>%
filter(str_detect(geographic_area_name, "Massachusetts")) %>%
mutate(
county = str_remove(geographic_area_name, " County,.*$"),
county = fct_reorder(county, income_estimate)
) %>%
ggplot(aes(county, income_estimate, color = category)) +
geom_errorbar(
aes(ymin = income_lo, ymax = income_hi),
position = position_dodge(width = wid)
) +
geom_point(position = position_dodge(width = wid)) +
coord_flip() +
labs(
x = "County",
y = "Median Household Income"
)
df_data %>%
ggplot(aes(x = population_estimate, y = income_SE, color = category, group = 1)) +
geom_point(alpha = 0.1) +
theme_minimal() +
scale_y_log10() +
scale_x_log10() +
geom_smooth()
