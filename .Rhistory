geom_errorbar(aes(
ymin = lo,
ymax = hi,
color = (lo <= 0.5) & (0.5 <= hi)
)) +
facet_grid(n~.) +
scale_color_discrete(name = "CI Contains True Mean") +
theme(legend.position = "bottom") +
labs(
x = "Replication",
y = "Estimated Mean"
)
## NOTE: No need to change this!
set.seed(101)
n_repl <- 5e3
df_clt <-
map_dfr(
1:n_repl,
function(id) {
map_dfr(
c(1, 2, 9, 81, 729),
function(n) {
tibble(
Z = runif(n),
n = n,
id = id
)
}
)
}
) %>%
group_by(n, id) %>%
summarize(mean = mean(Z), sd = sd(Z))
## NOTE: No need to change this!
df_clt %>%
filter(n > 1) %>%
mutate(
se = sd / sqrt(n),
lo = mean - z_c * se,
hi = mean + z_c * se,
flag = (lo <= 0.5) & (0.5 <= hi)
) %>%
group_by(n) %>%
summarize(coverage = mean(flag))
## NOTE: No need to change this!
df_flights_aa <-
flights %>%
filter(carrier == "AA") %>%
summarize(across(
arr_delay,
c(
"mean" = ~mean(., na.rm = TRUE),
"sd" = ~sd(., na.rm = TRUE),
"n" = ~length(.)
)
))
df_flights_aa
## NOTE: No need to change this!
set.seed(101)
# Downsample at different sample sizes, construct a confidence interval
df_flights_sampled <-
map_dfr(
c(5, 10, 25, 50, 100, 250, 500), # Sample sizes
function(n) {
flights %>%
filter(carrier == "AA") %>%
slice_sample(n = n) %>%
summarize(across(
arr_delay,
c(
"mean" = ~mean(., na.rm = TRUE),
"se" = ~sd(., na.rm = TRUE) / length(.)
)
)) %>%
mutate(
arr_delay_lo = arr_delay_mean - 1.96 * arr_delay_se,
arr_delay_hi = arr_delay_mean + 1.96 * arr_delay_se,
n = n
)
}
)
# Visualize
df_flights_sampled %>%
ggplot(aes(n, arr_delay_mean)) +
geom_hline(
data = df_flights_aa,
mapping = aes(yintercept = arr_delay_mean),
size = 0.1
) +
geom_hline(yintercept = 0, color = "white", size = 2) +
geom_errorbar(aes(
ymin = arr_delay_lo,
ymax = arr_delay_hi,
color = (0 < arr_delay_lo)
)) +
geom_point() +
scale_x_log10() +
scale_color_discrete(name = "Confidently Greater than Zero?") +
theme(legend.position = "bottom") +
labs(
x = "Observations",
y = "Arrival Delay (minutes)",
title = "American Airlines Delays"
)
library(tidyverse)
library(rsample)
## NOTE: No need to change this!
df_population <-
diamonds %>%
filter(carat < 1)
## NOTE: No need to change this!
set.seed(101)
df_sample <-
df_population %>%
slice_sample(n = 100)
## NOTE: No need to change this; this will be our decision threshold
price_threshold <- 1700
## NOTE: No need to change this; this will be our decision threshold
price_threshold <- 1700
library(tidyverse)
# knitr options
knitr::opts_chunk$set(echo = TRUE)
economics %>%
pivot_longer(
names_to = "variable",
values_to = "value",
cols = c(pce, pop, psavert, uempmed, unemploy)
) %>%
ggplot(aes(date, value)) +
geom_line() +
facet_wrap(~variable, scales = "free_y")
## NOTE: No need to edit; study this example
mpg %>%
ggplot(aes(displ, hwy)) +
geom_point() +
facet_wrap(~class)
## NOTE: No need to edit; study this example
mpg %>%
ggplot(aes(displ, hwy)) +
## A bit of a trick; remove the facet variable to prevent faceting
geom_point(
data = . %>% select(-class),
color = "grey80"
) +
geom_point() +
facet_wrap(~class) +
theme_minimal()
## TODO: Edit this code to facet on `cut`, but keep "ghost" points to aid in
## comparison.
diamonds %>%
ggplot(aes(carat, price)) +
geom_point(
data = . %>% select(-cut),
color = "grey80"
) +
geom_point() +
facet_wrap(~cut)
mpg %>%
group_by(model) %>%
filter(row_number(desc(year)) == 1) %>%
ungroup() %>%
mutate(
manufacturer = fct_reorder(manufacturer, hwy),
model = fct_reorder(model, desc(hwy))
) %>%
ggplot(aes(hwy, model)) +
geom_point() +
facet_grid(manufacturer~., scale = "free_y", space = "free") +
theme(
strip.text.y = element_text(angle = 0)
)
## TODO: Create a set of small multiples plot from these data
as_tibble(iris) %>%
pivot_longer(
names_to = "part",
values_to = "length",
cols = -Species
) %>%
ggplot(aes(length, Species)) +
geom_point() +
gacet_grid(part~., scale = "free_y", space = "free") +
theme(
strip.text.y = element_text(angle = 0)
)
## TODO: Create a set of small multiples plot from these data
as_tibble(iris) %>%
pivot_longer(
names_to = "part",
values_to = "length",
cols = -Species
) %>%
ggplot(aes(length, Species)) +
geom_point() +
facet_grid(part~., scale = "free_y", space = "free") +
theme(
strip.text.y = element_text(angle = 0)
)
# knitr options
knitr::opts_chunk$set(echo = TRUE)
## Note: No need to edit this chunk!
library(tidyverse)
library(googlesheets4)
url <- "https://docs.google.com/spreadsheets/d/1av_SXn4j0-4Rk0mQFik3LLr-uf0YdA06i3ugE6n-Zdo/edit?usp=sharing"
c_true <- 299792.458 # Exact speed of light in a vacuum (km / s)
c_michelson <- 299944.00  # Michelson's speed estimate (km / s)
meas_adjust <- +92 # Michelson's speed of light adjustment (km / s)
c_michelson_uncertainty <- 51 # Michelson's measurement uncertainty (km / s)
gs4_deauth()
ss <- gs4_get(url)
df_michelson <-
read_sheet(ss) %>%
select(Date, Distinctness, Temp, Velocity) %>%
mutate(
Distinctness = as_factor(Distinctness),
c_meas = Velocity + meas_adjust
)
## TASK: Compute `epsilon_c`
df_q1 <-
df_michelson %>%
mutate(epsilon_c = c_mean - c_true)
## TASK: Compute `epsilon_c`
df_q1 <-
df_michelson %>%
mutate(epsilon_c = c_meas - c_true)
df_q1 %>%
ggplot(aes(epsilon_c)) +
geom_histogram()
## TASK: Estimate `epsilon_mean` and `epsilon_sd` from df_q1
df_q2 <-
df_q1 %>%
summarise(
epsilon_mean = mean(epsilon_c),
epsilon_sd = sd(epsilon_c)
)
## TASK: Estimate `epsilon_mean` and `epsilon_sd` from df_q1
df_q2 <-
df_q1 %>%
summarise(
epsilon_mean = mean(epsilon_c),
epsilon_sd = sd(epsilon_c)
)
View(df_q1)
## TASK: Estimate `epsilon_mean` and `epsilon_sd` from df_q1
df_q2 <-
df_q1 %>%
summarize(
epsilon_mean = mean(epsilon_c),
epsilon_sd = sd(epsilon_c)
)
## TASK: Estimate `epsilon_mean` and `epsilon_sd` from df_q1
df_q2 <-
df_q1 %>%
summarize(
epsilon_mean = mean(epsilon_c),
epsilon_sd = sd(epsilon_c)
)
View(df_q2)
## NOTE: No need to change this!
assertthat::assert_that(abs((df_q2 %>% pull(epsilon_mean)) - 151.942) < 1e-3)
assertthat::assert_that(abs((df_q2 %>% pull(epsilon_sd)) - 79.01055) < 1e-3)
print("Great job!")
## TASK: Compute a 99% confidence interval on the mean of c_meas
C <- 0.99
q <- qnorm( 1 - (1 - C) / 2 )
df_q3 <-
df_q1 %>%
summarize(
c_meas_mean = mean(c_meas),
c_meas_sd = sd(c_meas),
n_samp = n(),
c_lo = c_meas_mean - q * c_meas_sd / sqrt(n_samp),
c_hi = c_meas_mean + q * c_meas_sd / sqrt(n_samp)
)
## NOTE: This checks if the CI contains c_true
(df_q3 %>% pull(c_lo) <= c_true) & (c_true <= df_q3 %>% pull(c_hi))
## TASK: Compute a confidence interval on the mean, use to answer the question
## above
df_sample %>%
summarize(
price_mean = mean(price),
price_sd = sd(price),
price_lo = price_mean - 1.96 * price_sd / sqrt(n()),
price_hi = price_mean + 1.96 * price_sd / sqrt(n())
) %>%
select(price_lo, price_hi)
## TASK: Compute a confidence interval on the mean, use to answer the question
## above
df_sample %>%
summarize(
price_mean = mean(price),
price_sd = sd(price),
price_lo = price_mean - 1.96 * price_sd / sqrt(n()),
price_hi = price_mean + 1.96 * price_sd / sqrt(n())
) %>%
select(price_lo, price_hi)
price_threshold
## TASK: Estimate a confidence interval for the proportion of high-cut diamonds
## in the population. Look to `e-stat09-bootstrap` for starter code.
set.seed(101)
alpha <- 0.01
fit_fun <- function(split_df) {
analysis(split_df) %>%
summarize(estimate = mean((cut == "Premium") | (cut == "Ideal"))) %>%
pull(estimate)
}
df_sample %>%
bootstraps(., times = 1000) %>%
mutate(p_hat = map_dbl(splits, fit_fun)) %>%
summarize(
p_lo = quantile(p_hat, alpha / 2),
p_up = quantile(p_hat, 1 - alpha / 2),
)
## TASK: Compute the population mean of diamond price
df_population %>%
summarize(price = mean(price))
price_threshold
library(tidyverse)
library(rsample)
## TASK: Compute a bootstrap confidence interval at the 95% level (alpha = 0.05)
z_c <- qnorm( 1 - (1 - 0.95) / 2 )
df_q5 <-
df_q4 %>%
summarize(
pi_lo = mean(pi_est) - z_c * sd(pi_est) / sqrt(length(pi_est))
#pi_up = ?
)
library(tidyverse)
library(rsample)
## NOTE: No need to edit; this visual helps explain the pi estimation scheme
tibble(x = seq(0, 1, length.out = 100)) %>%
mutate(y = sqrt(1 - x^2)) %>%
ggplot(aes(x, y)) +
annotate(
"rect",
xmin = 0, ymin = 0, xmax = 1, ymax = 1,
fill = "grey40",
size = 1
) +
geom_ribbon(aes(ymin = 0, ymax = y), fill = "coral") +
geom_line() +
annotate(
"label",
x = 0.5, y = 0.5, label = "Sc",
size = 8
) +
annotate(
"label",
x = 0.8, y = 0.8, label = "St",
size = 8
) +
scale_x_continuous(breaks = c(0, 1/2, 1)) +
scale_y_continuous(breaks = c(0, 1/2, 1)) +
theme_minimal() +
coord_fixed()
## TASK: Choose a sample size and generate samples
n <- 100000 # Choose a sample size
x <- runif(n, min = 0, max = 1)
y <- runif(n, min = 0, max = 1)
df_q1 <-
tibble(x,y)
df_q1
## NOTE: Do not edit this code
# Correct sample size
assertthat::assert_that(
dim(df_q1) %>%
.[[1]] == n,
msg = "The sample size should be `n`"
)
# Correct column names
assertthat::assert_that(
setequal(names(df_q1), c("x", "y")),
msg = "df_q1 must include the columns `x` and `y`"
)
print("Good")
## TASK: Finish implementing this function
stat <- function(x, y) {
r <- sqrt(x^2 + y^2)
in_circle <- r <= 1
stat <- 4 * in_circle
}
## TASK: Finish writing these assert statements
# Check the value for points *inside* the circle
assertthat::assert_that(
tibble(x = 0, y = 0) %>% # Pick a point *inside* the circle
mutate(stat = stat(x, y)) %>%
pull(stat) %>%
.[[1]] == 4,
# ???, # Write the correct value of stat() here
msg = "Incorrect value when a point is inside the circle"
)
# Check the value for points *outside* the circle
assertthat::assert_that(
tibble(x = 1, y = 1) %>% # Pick a point *outside* the circle
mutate(stat = stat(x, y)) %>%
pull(stat) %>%
.[[1]] == 0,
# ???, # Write the correct value of stat() here
msg = "Incorrect value when a point is outside the circle"
)
print("Your assertions passed, but make sure they're checking the right thing!")
## TASK: Estimate pi using your data from q1
df_q3 <-
df_q1 %>%
# mutate(sum_sq = x^2 + y^2) %>%
# filter(sum_sq <= 1)%>%
#summarise(pi_est = 4 * length(sum_sq)/n)
mutate(stat = stat(x,y)) %>%
summarize(pi_est = mean(stat))
df_q3
## NOTE: Do not edit this code
# Correct sample size
assertthat::assert_that(
dim(df_q3) %>%
.[[1]] == 1,
msg = "This result should have just one row"
)
# Correct column names
assertthat::assert_that(
setequal(names(df_q3), c("pi_est")),
msg = "df_q3 must include the column `pi_est`"
)
print("Good")
## TASK: Finish the code below
df_q4 <-
df_q1 %>%
bootstraps(., times = 1000) %>%
mutate(
pi_est = map_dbl(
splits,
function(split_df) {
analysis(split_df) %>%
# Estimate pi (pi_est) using the resampled data;
# this should be *identical* to the
# code you wrote for q3
mutate(stat = stat(x,y)) %>%
summarize(pi_est = mean(stat)) %>%
pull(pi_est)
}
)
)
## NOTE: Do not edit; use this to visualize your results
df_q4 %>%
ggplot(aes(pi_est)) +
geom_histogram()
## TASK: Compute a bootstrap confidence interval at the 95% level (alpha = 0.05)
z_c <- qnorm( 1 - (1 - 0.95) / 2 )
df_q5 <-
df_q4 %>%
summarize(
pi_lo = mean(pi_est) - z_c * sd(pi_est) / sqrt(length(pi_est))
#pi_up = ?
)
df_q5
## TASK: Compute a bootstrap confidence interval at the 95% level (alpha = 0.05)
z_c <- qnorm( 1 - (1 - 0.95) / 2 )
df_q5 <-
df_q4 %>%
summarize(
pi_lo = mean(pi_est) - z_c * sd(pi_est) / sqrt(length(pi_est)),
pi_high = mean(pi_est) + z_c * sd(pi_est) / sqrt(length(pi_est))
)
df_q5
## TASK: Compute a bootstrap confidence interval at the 95% level (alpha = 0.05)
z_c <- qnorm( 1 - (1 - 0.80) / 2 )
df_q5 <-
df_q4 %>%
summarize(
pi_lo = mean(pi_est) - z_c * sd(pi_est) / sqrt(length(pi_est)),
pi_hi = mean(pi_est) + z_c * sd(pi_est) / sqrt(length(pi_est))
)
df_q5
## TASK: Compute a bootstrap confidence interval at the 95% level (alpha = 0.05)
z_c <- qnorm( 1 - (1 - 0.85) / 2 )
df_q5 <-
df_q4 %>%
summarize(
pi_lo = mean(pi_est) - z_c * sd(pi_est) / sqrt(length(pi_est)),
pi_hi = mean(pi_est) + z_c * sd(pi_est) / sqrt(length(pi_est))
)
df_q5
z_c <- qnorm( 1 - (1 - 0.95) / 2 )
sd(pi_est)
## TASK: Compute a bootstrap confidence interval at the 95% level (alpha = 0.05)
z_c <- qnorm( 1 - (1 - 0.95) / 2 )
df_q5 <-
df_q4 %>%
summarize(
sd_p = sd(pi_est),
pi_lo = mean(pi_est) - z_c * sd(pi_est) / sqrt(length(pi_est)),
pi_hi = mean(pi_est) + z_c * sd(pi_est) / sqrt(length(pi_est))
)
df_q5
df_q1 %>%
mutate(stat = stat(x,y)) %>%
summarize(
mean_pi = mean(stat),
sd_pi = sd(stat),
z_c = qnorm( 1 - (1 - 0.95) / 2 ),
pi_lo = mean_pi - z_c * sd_pi / sqrt(length(stat)),
pi_hi = mean_pi + z_c * sd_pi / sqrt(length(stat))
)
View(df_q4)
View(df_q4)
## TASK: Compute a bootstrap confidence interval at the 95% level (alpha = 0.05)
z_c <- qnorm( 1 - (1 - 0.95) / 2 )
df_q5 <-
df_q4 %>%
summarize(
sd_p = sd(stat),
pi_lo = mean(pi_est) - z_c * sd(pi_est) / sqrt(length(pi_est)),
pi_hi = mean(pi_est) + z_c * sd(pi_est) / sqrt(length(pi_est))
)
