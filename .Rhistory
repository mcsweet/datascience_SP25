df_top10_cases_2 <-
df_normalized %>%
group_by(date) %>%
filter(date == max(date)) %>%
arrange(desc(cases_per100k)) %>%
slice(0:10)
## TASK: Find the top 10 deaths_per100k counties; report populations as well
df_top10_deaths_2 <-
df_normalized %>%
group_by(date) %>%
filter(date == max(date)) %>%
arrange(desc(deaths_per100k)) %>%
slice(0:10)
df_top10_cases_2
df_top10_deaths_2
## TASK: Find the top 10 max cases_per100k counties; report populations as well
df_top10_cases <-
df_normalized %>%
group_by(county) %>%
summarise(across(c(cases_per100k, population), max)) %>%
arrange(desc(cases_per100k)) %>%
slice(0:10)
## TASK: Find the top 10 deaths_per100k counties; report populations as well
df_top10_deaths <-
df_normalized %>%
group_by(county) %>%
summarise(across(c(deaths_per100k, population), max)) %>%
arrange(desc(deaths_per100k)) %>%
slice(0:10)
df_top10_cases
df_top10_deaths
## TASK: Find the top 10 max cases_per100k counties; report populations as well
df_top10_cases_2 <-
df_normalized %>%
group_by(fips) %>%
filter(date == max(date)) %>%
arrange(desc(cases_per100k)) %>%
slice(0:10)
## TASK: Find the top 10 deaths_per100k counties; report populations as well
df_top10_deaths_2 <-
df_normalized %>%
group_by(fips) %>%
filter(date == max(date)) %>%
arrange(desc(deaths_per100k)) %>%
slice(0:10)
df_top10_cases_2
df_top10_deaths_2
## TASK: Find the top 10 max cases_per100k counties; report populations as well
df_top10_cases <-
df_normalized %>%
group_by(county) %>%
summarise(across(c(cases_per100k, population), max)) %>%
arrange(desc(cases_per100k)) %>%
slice(0:10)
## TASK: Find the top 10 deaths_per100k counties; report populations as well
df_top10_deaths <-
df_normalized %>%
group_by(county) %>%
summarise(across(c(deaths_per100k, population), max)) %>%
arrange(desc(deaths_per100k)) %>%
slice(0:10)
df_top10_cases
df_top10_deaths
## TASK: Find the top 10 max cases_per100k counties; report populations as well
df_top10_cases_2 <-
df_normalized %>%
group_by(fips) %>%
filter(date == max(date)) %>%
summarise(across(c(cases_per100k, population), max)) %>%
arrange(desc(cases_per100k)) %>%
slice(0:10)
## TASK: Find the top 10 deaths_per100k counties; report populations as well
df_top10_deaths_2 <-
df_normalized %>%
group_by(fips) %>%
filter(date == max(date)) %>%
arrange(desc(deaths_per100k)) %>%
slice(0:10)
df_top10_cases_2
df_top10_deaths_2
## TASK: Find the top 10 max cases_per100k counties; report populations as well
df_top10_cases <-
df_normalized %>%
group_by(county) %>%
summarise(across(c(cases_per100k, population), max)) %>%
arrange(desc(cases_per100k)) %>%
slice(0:10)
## TASK: Find the top 10 deaths_per100k counties; report populations as well
df_top10_deaths <-
df_normalized %>%
group_by(county) %>%
summarise(across(c(deaths_per100k, population), max)) %>%
arrange(desc(deaths_per100k)) %>%
slice(0:10)
df_top10_cases
df_top10_deaths
## TASK: Find the top 10 max cases_per100k counties; report populations as well
df_top10_cases_2 <-
df_normalized %>%
group_by(fips) %>%
filter(date == max(date)) %>%
summarise(across(c(date, cases_per100k, population), max)) %>%
arrange(desc(cases_per100k)) %>%
slice(0:10)
## TASK: Find the top 10 deaths_per100k counties; report populations as well
df_top10_deaths_2 <-
df_normalized %>%
group_by(fips) %>%
filter(date == max(date)) %>%
arrange(desc(deaths_per100k)) %>%
slice(0:10)
df_top10_cases_2
df_top10_deaths_2
## TASK: Find the top 10 max cases_per100k counties; report populations as well
df_top10_cases <-
df_normalized %>%
group_by(county) %>%
summarise(across(c(cases_per100k, population), max)) %>%
arrange(desc(cases_per100k)) %>%
slice(0:10)
## TASK: Find the top 10 deaths_per100k counties; report populations as well
df_top10_deaths <-
df_normalized %>%
group_by(county) %>%
summarise(across(c(deaths_per100k, population), max)) %>%
arrange(desc(deaths_per100k)) %>%
slice(0:10)
df_top10_cases
df_top10_deaths
## TASK: Find the top 10 max cases_per100k counties; report populations as well
df_top10_cases_2 <-
df_normalized %>%
group_by(fips) %>%
filter(date == max(date)) %>%
summarise(across(c(date, cases_per100k, population), max)) %>%
arrange(desc(cases_per100k)) %>%
slice(0:10)
## TASK: Find the top 10 deaths_per100k counties; report populations as well
df_top10_deaths_2 <-
df_normalized %>%
group_by(fips) %>%
filter(date == max(date)) %>%
summarise(across(c(date, deaths_per100k, population), max)) %>%
arrange(desc(deaths_per100k)) %>%
slice(0:10)
df_top10_cases_2
df_top10_deaths_2
library(tidyverse)
## NOTE: No need to change; run this chunk
df_stang_long %>%
ggplot(aes(nu, E, color = as_factor(thick))) +
geom_point(size = 3) +
theme_minimal()
library(tidyverse)
## NOTE: If you extracted all challenges to the same location,
## you shouldn't have to change this filename
filename <- "./data/stang.csv"
## Load the data
df_stang <- read_csv(filename)
df_stang
## TASK: Tidy `df_stang`
df_stang_long <-
df_stang %>%
pivot_longer(
names_to = c(".value","angle"),
names_sep = "_",
cols = c(-thick, -alloy)
) %>%
filter(E > 0) %>%
transform(angle = as.integer(angle))
df_stang_long
## NOTE: No need to change this
## Names
assertthat::assert_that(
setequal(
df_stang_long %>% names,
c("thick", "alloy", "angle", "E", "nu")
)
)
## Dimensions
assertthat::assert_that(all(dim(df_stang_long) == c(26, 5)))
## Type
assertthat::assert_that(
(df_stang_long %>% pull(angle) %>% typeof()) == "integer"
)
print("Very good!")
## Is there "One True Value"
glimpse(df_stang_long)
## Get the Number of Aluminum alloys
df_stang_long %>%
distinct(alloy)
## What angles?
df_stang_long %>%
distinct(angle)
## What thicknesses?
df_stang_long %>%
distinct(thick)
## Mean Poisson's Ratio of al_24st
df_stang_long %>%
group_by(angle) %>%
summarise(E = mean(E))
## TASK: Investigate your question from q1 here
## transform angle values from integeter to characters
df_stang_plot <-
df_stang_long %>%
transform(angle = as.character(angle))
## summarise the median and mean of E for each angle
df_stang_long %>%
group_by(angle) %>%
summarise(
Median_E = median(E),
Mean_E = mean(E)
)
## scatter plot of angle and youngs modulus
df_stang_plot %>%
group_by(angle) %>%
ggplot(aes(x = angle, y = E, fill = angle)) +
geom_point()
df_stang_plot %>%
group_by(angle) %>%
ggplot(aes(x = angle, y = E, fill = angle)) +
geom_boxplot()
## NOTE: No need to change; run this chunk
df_stang_long %>%
ggplot(aes(nu, E, color = as_factor(thick))) +
geom_point(size = 3) +
theme_minimal()
library(tidyverse)
library(readxl) # For reading Excel sheets
library(tidyverse)
library(modelr)
library(broom)
## Helper function to compute uncertainty bounds
add_uncertainties <- function(data, model, prefix = "pred", ...) {
df_fit <-
stats::predict(model, data, ...) %>%
as_tibble() %>%
rename_with(~ str_c(prefix, "_", .))
bind_cols(data, df_fit)
}
library(tidyverse)
library(modelr)
library(broom)
## Helper function to compute uncertainty bounds
add_uncertainties <- function(data, model, prefix = "pred", ...) {
df_fit <-
stats::predict(model, data, ...) %>%
as_tibble() %>%
rename_with(~ str_c(prefix, "_", .))
bind_cols(data, df_fit)
}
## TODO: Perform your initial checks
glimpse(df_psaap)
## NOTE: No need to edit this chunk
## Download PSAAP II data and unzip
url_zip <- "https://ndownloader.figshare.com/files/24111269"
filename_zip <- "./data/psaap.zip"
filename_psaap <- "./data/psaap.csv"
curl::curl_download(url_zip, destfile = filename_zip)
unzip(filename_zip, exdir = "./data")
df_psaap <- read_csv(filename_psaap)
## TODO: Perform your initial checks
glimpse(df_psaap)
head(df_psaap)
## TODO: Perform your initial checks
summarise(df_psaap)
head(df_psaap)
## TODO: Perform your initial checks
summary(df_psaap)
head(df_psaap)
## TODO: Visualize the data in df_psaap with T_norm against x;
##       design your visual to handle the multiple simulations,
##       each identified by different values of idx
df_psaap %>%
ggplot(aes(x = x, y = T_norm, group = idx, color = factor(idx))) +
geom_line()
## TODO: Visualize the data in df_psaap with T_norm against x;
##       design your visual to handle the multiple simulations,
##       each identified by different values of idx
df_psaap %>%
ggplot(aes(x = x, y = T_norm, group = idx, color = factor(idx))) +
geom_line() +
labs(
title = "Norm Temp Rise vs. X Location",
x = "Channel Locations (x)",
y = "Normalized Temp Rise"
)
## NOTE: No need to edit this chunk
# Addl' Note: These data are already randomized by idx; no need
# to additionally shuffle the data!
df_train <- df_psaap %>% filter(idx %in% 1:20)
df_validate <- df_psaap %>% filter(idx %in% 21:36)
## NOTE: No need to edit these models
fit_baseline <-
df_train %>%
lm(formula = T_norm ~ x)
fit_cheat <-
df_train %>%
lm(formula = T_norm ~ avg_T)
fit_nonphysical <-
df_train %>%
lm(formula = T_norm ~ idx)
## TODO: Compute a measure of accuracy for each fit above;
##       compare their relative performance
## NOTE: No need to edit these models
fit_baseline <-
df_train %>%
lm(formula = T_norm ~ x)
fit_cheat <-
df_train %>%
lm(formula = T_norm ~ avg_T)
fit_nonphysical <-
df_train %>%
lm(formula = T_norm ~ idx)
## TODO: Compute a measure of accuracy for each fit above;
##       compare their relative performance
mse(fit_baseline, df_validate)
## NOTE: No need to edit these models
fit_baseline <-
df_train %>%
lm(formula = T_norm ~ x)
fit_cheat <-
df_train %>%
lm(formula = T_norm ~ avg_T)
fit_nonphysical <-
df_train %>%
lm(formula = T_norm ~ idx)
## TODO: Compute a measure of accuracy for each fit above;
##       compare their relative performance
Mse_Baseline <- mse(fit_baseline, df_validate)
Mse_cheat <- mse(fit_cheat, df_validate)
Mse_nonphysical <- mse(fit_nonphysical, df_validate)
## NOTE: No need to edit these models
fit_baseline <-
df_train %>%
lm(formula = T_norm ~ x)
fit_cheat <-
df_train %>%
lm(formula = T_norm ~ avg_T)
fit_nonphysical <-
df_train %>%
lm(formula = T_norm ~ idx)
## TODO: Compute a measure of accuracy for each fit above;
##       compare their relative performance
Mse_Baseline <- mse(fit_baseline, df_validate)
Mse_cheat <- mse(fit_cheat, df_validate)
Mse_nonphysical <- mse(fit_nonphysical, df_validate)
Mse_Baseline
Mse_cheat
Mse_nonphysical
## NOTE: No need to edit these models
fit_baseline <-
df_train %>%
lm(formula = T_norm ~ x)
fit_cheat <-
df_train %>%
lm(formula = T_norm ~ avg_T)
fit_nonphysical <-
df_train %>%
lm(formula = T_norm ~ idx)
## TODO: Compute a measure of accuracy for each fit above;
##       compare their relative performance
Mse_Baseline <- rmse(fit_baseline, df_validate)
Mse_cheat <- rmse(fit_cheat, df_validate)
Mse_nonphysical <- rmse(fit_nonphysical, df_validate)
Mse_Baseline
Mse_cheat
Mse_nonphysical
## TODO: Inspect the regression coefficients for the following model
fit_q4 <-
df_train %>%
# lm(formula = T_norm ~ . - idx - avg_q - avg_T - rms_T)
lm(formula = T_norm ~ L + W + U_0 + N_p + k_f + T_f)
# lm(formula = T_norm ~ L - W - U_0 - N_p - k_f - T_f)
## TODO: Inspect the regression coefficients for the following model
fit_q4 <-
df_train %>%
# lm(formula = T_norm ~ . - idx - avg_q - avg_T - rms_T)
lm(formula = T_norm ~ L + W + U_0 + N_p + k_f + T_f)
# lm(formula = T_norm ~ L - W - U_0 - N_p - k_f - T_f)
sd(x) = 0.2805121
## TODO: Inspect the regression coefficients for the following model
fit_q4 <-
df_train %>%
# lm(formula = T_norm ~ . - idx - avg_q - avg_T - rms_T)
lm(formula = T_norm ~ L + W + U_0 + N_p + k_f + T_f)
# lm(formula = T_norm ~ L - W - U_0 - N_p - k_f - T_f)
sd(df_psaap$x)
sd(df_psaap$T_f)
## NOTE: No need to edit this chunk
fit_simple <-
df_train %>%
lm(data = ., formula = T_norm ~ x)
df_intervals <-
df_train %>%
add_uncertainties(fit_simple, interval = "confidence", prefix = "ci") %>%
add_uncertainties(fit_simple, interval = "prediction", prefix = "pi")
## NOTE: No need to edit this chunk
df_intervals %>%
select(T_norm, x, matches("ci|pi")) %>%
pivot_longer(
names_to = c("method", ".value"),
names_sep = "_",
cols = matches("ci|pi")
) %>%
ggplot(aes(x, fit)) +
geom_errorbar(
aes(ymin = lwr, ymax = upr, color = method),
width = 0.05,
size = 1
) +
geom_smooth(
data = df_psaap %>% mutate(method = "ci"),
mapping = aes(x, T_norm),
se = FALSE,
linetype = 2,
color = "black"
) +
geom_point(
data = df_validate %>% mutate(method = "pi"),
mapping = aes(x, T_norm),
size = 0.5
) +
facet_grid(~method) +
theme_minimal() +
labs(
x = "Channel Location (-)",
y = "Normalized Temperature Rise (-)"
)
## TODO: Run this code and interpret the results
## NOTE: No need to edit this chunk
## NOTE: This chunk will use your model from q4; it will predict on the
##       validation data, add prediction intervals for every prediction,
##       and visualize the results on a predicted-vs-actual plot. It will
##       also compare against the simple `fit_simple` defined above.
bind_rows(
df_psaap %>%
add_uncertainties(fit_simple, interval = "prediction", prefix = "pi") %>%
select(T_norm, pi_lwr, pi_fit, pi_upr) %>%
mutate(model = "x only"),
df_psaap %>%
add_uncertainties(fit_q4, interval = "prediction", prefix = "pi") %>%
select(T_norm, pi_lwr, pi_fit, pi_upr) %>%
mutate(model = "q4"),
) %>%
ggplot(aes(T_norm, pi_fit)) +
geom_abline(slope = 1, intercept = 0, color = "grey80", size = 2) +
geom_errorbar(
aes(ymin = pi_lwr, ymax = pi_upr),
width = 0
) +
geom_point() +
facet_grid(~ model, labeller = label_both) +
theme_minimal() +
labs(
title = "Predicted vs Actual",
x = "Actual T_norm",
y = "Predicted T_norm"
)
# NOTE: No need to change df_design; this is the target the client
#       is considering
df_design <- tibble(x = 1, L = 0.2, W = 0.04, U_0 = 1.0)
# NOTE: This is the level the "probability" level customer wants
pr_level <- 0.8
## TODO: Fit a model, assess the uncertainty in your prediction,
#        use the validation data to check your uncertainty estimates, and
#        make a recommendation on a *dependable range* of values for T_norm
#        at the point `df_design`
fit_q6 <- NA
# NOTE: No need to change df_design; this is the target the client
#       is considering
df_design <- tibble(x = 1, L = 0.2, W = 0.04, U_0 = 1.0)
# NOTE: This is the level the "probability" level customer wants
pr_level <- 0.8
## TODO: Fit a model, assess the uncertainty in your prediction,
#        use the validation data to check your uncertainty estimates, and
#        make a recommendation on a *dependable range* of values for T_norm
#        at the point `df_design`
fit_q6 <- NA
# NOTE: No need to change df_design; this is the target the client
#       is considering
df_design <- tibble(x = 1, L = 0.2, W = 0.04, U_0 = 1.0)
# NOTE: This is the level the "probability" level customer wants
pr_level <- 0.8
## TODO: Fit a model, assess the uncertainty in your prediction,
#        use the validation data to check your uncertainty estimates, and
#        make a recommendation on a *dependable range* of values for T_norm
#        at the point `df_design`
fit_q6 <- lm(T_norm ~ N_p + k_f + T_f + rho_f + mu_f + lam_f + C_fp + rho_p + d_p + C_pv + h + I_0 + eps_p, data = df_train)
df_predict <- df_psaap %>%
select(N_p, k_f, T_f, rho_f, mu_f, lam_f, C_fp, rho_p, d_p, C_pv, h, I_0, eps_p) %>%
mutate(
x = 1.0,
L = 0.2,
W = 0.04,
U_0 = 1.0
) %>%
select(x, L, W, U_0, everything())
df_results <- df_predict %>%
add_uncertainties(fit_q6, interval = "prediction", prefix = "pi", level = pr_level)
range_pi <- range(df_results$pi_lwr, df_results$pi_upr)
mean_pred <- mean(df_results$pi_fit)
tibble(
Lower80 = range_pi[1],
Upper80 = range_pi[2],
MeanPredictedTnorm = mean_pred
)
df_validate_pi <- df_validate %>%
add_uncertainties(fit_q6, interval = "prediction", prefix = "pi", level = pr_level)
within_interval <- df_validate_pi %>%
mutate(within = T_norm >= pi_lwr & T_norm <= pi_upr)
mean(within_interval$within)
